# Date
20/05/2021

# Agenda
## Tasks completed for the past week
1. Learnt background knowledge about computer architecture (memory hierarchy, pipeline, vector architecture and SIMD)
2. Finished 55% of Mehedi's thesis

## My outlook about this project
At least three potential directions:

1. Further optimise the sparse SMM methods from Mehedi: get performace closer to the roofline.
2. Make Mehedi's method compatible with older AVX instructions. (maybe more direction stem from Mehedi's work, need to finish his thesis)
3. Apply similar SMM methods for different architectures (e.g. ARM SVE, RISC-V RVV, or GPUs)

## Plan for the near weeks
Read into the relevent field and choose one direction which will bring the maximum value.

### Interesting area for reading

1. Libraries for sparse small MM computations (LIBXSMM, GiMMiK, VecReg ...)
2. Software that might benefits from SSMM (PyFR, neural networks, other FEAs ...)
3. The common architectures optimised for scientific computings
4. Other potential beneficial low-level optimisation techniques

## Things I would like to include in my interim report

1. Mehedi's thesis.
2. Points 1-3 above.
3. Project schedule.

It is difficult to give a comprehensive discussion about all the content, but I'll try my best.
Plan to send the table of contents by this Friday and prepare a draft by our meeting next week.


## Questions
1. In the thesis, Mehedi mentioned that LIBXSMM is threading-runtime agnostic. What is the significance of this?
2. How should we deal with the non-deterministics during the testing process:
    - OS -> context switching
    - Matrix size -> cache usages
    - CPU temp -> clock speed (if the other cores are heavily loaded)

    Mehedi use `gettimeofday` to measure the cpu time for the small MM computation. Should we use `perf ` to count the cpu cycles?

3. Where can I learn about details of the instruction set of an architecture/cpu (e.g. latency, CPI, # of lanes, register size, cache/RAM bandwidth ...)
